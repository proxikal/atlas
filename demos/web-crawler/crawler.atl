// Crawler module - BFS web crawling using Queue and HashSet
import { http_get, http_is_success, http_body } from "std:http";
import { queueNew, queuePush, queuePop, queueIsEmpty, queueSize } from "std:collections";
import { setNew, setAdd, setHas } from "std:collections";
import { mapNew, mapSet, mapGet, mapHas } from "std:collections";
import { extractLinks, extractTitle, extractDescription } from "./extractor";
import { normalizeUrl, makeAbsolute, shouldCrawl, extractDomain, isValidUrl } from "./urls";

// Page data structure
fn createPageData(url: string, title: string, description: string, linkCount: number) => JsonValue {
    return jsonObject([
        ["url", jsonString(url)],
        ["title", jsonString(title)],
        ["description", jsonString(description)],
        ["linkCount", jsonNumber(linkCount)]
    ]);
}

// Crawl result structure
export fn createCrawlResult(pages: array, linkGraph: HashMap, stats: JsonValue) => JsonValue {
    return jsonObject([
        ["pages", jsonArray(pages)],
        ["stats", stats]
    ]);
}

// Fetch and parse a single page
fn fetchPage(url: string) => Result<string, string> {
    let response: Result<HttpResponse, string> = http_get(url);

    match response {
        Ok(resp) => {
            if http_is_success(resp) {
                return Ok(http_body(resp));
            } else {
                return Err("HTTP error");
            }
        },
        Err(e) => return Err(e)
    }
}

// Main crawl function (BFS traversal)
export fn crawl(startUrl: string, maxPages: number) => Result<JsonValue, string> {
    let baseDomain: string = extractDomain(startUrl);
    let normalizedStart: string = normalizeUrl(startUrl);

    // Collections for BFS
    let queue: Queue = queueNew();
    let visited: HashSet = setNew();
    let pages: array = [];
    let linkGraph: HashMap = mapNew();

    // Initialize
    queuePush(queue, normalizedStart);

    let pagesVisited: number = 0;

    while !queueIsEmpty(queue) && pagesVisited < maxPages {
        let currentOpt: Option<JsonValue> = queuePop(queue);

        let current: string = match currentOpt {
            Some(url) => jsonAsString(url),
            None => break
        };

        // Skip if already visited
        if setHas(visited, current) {
            continue;
        }

        setAdd(visited, current);

        // Fetch page
        print("üîç Crawling: " + current);

        let htmlResult: Result<string, string> = fetchPage(current);

        match htmlResult {
            Ok(html) => {
                // Extract data
                let title: string = extractTitle(html);
                let description: string = extractDescription(html);
                let links: array = extractLinks(html);

                let validLinks: array = [];

                // Process links
                for link in links {
                    let linkStr: string = link;

                    // Convert to absolute URL
                    let absolute: string = makeAbsolute(current, linkStr);
                    let normalized: string = normalizeUrl(absolute);

                    // Only crawl same domain and valid URLs
                    if shouldCrawl(normalized, baseDomain) && isValidUrl(normalized) {
                        if !setHas(visited, normalized) {
                            queuePush(queue, normalized);
                        }
                        push(validLinks, normalized);
                    }
                }

                // Store page data
                let pageData: JsonValue = createPageData(current, title, description, len(validLinks));
                push(pages, pageData);

                // Store link graph
                mapSet(linkGraph, current, jsonArray(validLinks));

                pagesVisited = pagesVisited + 1;
            },
            Err(e) => {
                print("  ‚ùå Failed: " + e);
            }
        }
    }

    // Create statistics
    let stats: JsonValue = jsonObject([
        ["pagesVisited", jsonNumber(pagesVisited)],
        ["totalLinks", jsonNumber(0)], // Would calculate from linkGraph
        ["startUrl", jsonString(startUrl)],
        ["domain", jsonString(baseDomain)]
    ]);

    return Ok(createCrawlResult(pages, linkGraph, stats));
}
